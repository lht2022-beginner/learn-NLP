# RNN
## 处理序列信息的其他方法
+ 词袋模型：忽略语序
+ 全连接网络(FCN)：可先求和或先全连接变换。但为处理变长的句子，每个时间步使用的全连接层必须共享同一套权重参数。每个词元孤立、无序列特征。
    > 在处理序列数据（如文本）时，模型按顺序逐个处理序列中的元素（如词元）。**时间步**是这个处理过程中的一个离散步骤。在文本处理的上下文中，一个时间步通常对应于处理一个词元。例如，对于句子“播放周杰伦的歌”，处理“播放”是时间步1，处理“周杰伦”是时间步2，以此类推。
+ 卷积神经网络(CNN)：使用一维卷积核（窗口）划过整个词向量序列，可捕捉局部依赖关系。但感受野固定，长距离依赖难捕捉、最佳窗口大小难确定。
## RNN基础结构和工作原理
记住在处理当前词元前看到了哪些信息。
$h_t$=tanh($Ux_t+Wh_{t-1}+b$)

$Ux_t$为当前输入信息，权重矩阵×当前时间步的输入向量；$Wh_{t-1}$为上一时间步记忆，循环权重矩阵×上一步隐藏状态；$h_t$为当前步的隐藏状态。$b$为偏置项。所有时间步权重矩阵$W$和$U$共享。

当 RNN 处理完最后一个词元后，得到的**最终隐藏状态就被认为是整个句子的动态上下文表示**，即我们需要的 “文本向量”。这个向量捕获了整个句子的语序和语义信息。最后，我们将这个向量送入一个标准的**全连接分类层**，就可以得到**各个类别的置信度**了。通过这种方式，RNN 成功地将一个变长的词向量序列，编码成了一个蕴含序列信息的固定长度的特征向量。

通过$h_t$实现了静态Type到动态Token的跨越
> 这里的 token 并不是词元的概念。在语言学中，Type（类型）指抽象的、通用的概念，而 Token（标记/实例）指该概念在特定时空下的具体表现。例如，“我一边玩黑神话悟空，一边等黑神话钟馗”这句话中，有 2 个“黑神话”的 Token，但只有一个“黑神话”的 Type。
## 局限性
+ 梯度消失、梯度爆炸：BPPT链式求导而$W$的范数小于1、大于1——梯度裁剪
+ 长距离依赖无法捕捉：梯度消失导致
+ 单向性：无法利用未来上下文信息——LSTM、GRU

# LSTM
## 门控机制
让模型在训练中学会**有选择地**让信息通过、遗忘旧信息或输出信息。
### 双轨并行
2个独立状态向量随时间并行传递
+ 细胞状态$c_t$，直接传递长期记忆
+ 隐藏状态$h_t$，与RNN类似，短期记忆和最终输出
### 门结构
让信息选择性通过，是全连接层。输入$x_t$和$h_{t-1}$经sigmoid，输出(0,1)区间的向量
## 单元结构
![CUDA版本](images/3_2_1.svg)
### 第一步：遗忘门（Forget Gate）

决定我们从细胞状态中丢弃什么信息。这个决定由被称为“遗忘门”的 Sigmoid 层完成。它会审视 $h_{t-1}$ 和 $x_t$，然后为 $c_{t-1}$ 中的每个数值输出一个介于 0 和 1 之间的“遗忘系数”。当该系数接近 1 时表示“几乎完全保留”，当其接近 0 时表示“几乎完全丢弃”。

$$
f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
$$


### 第二步：输入门与候选记忆（Input Gate & Candidate Memory）

下一步是决定我们在细胞状态中存储什么新信息。这由两部分共同完成：

（1）**输入门**：首先，一个 Sigmoid 层（即图中的“输入门”）决定了更新哪些值。

$$
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
$$

（2）**候选记忆**：然后，一个 `tanh` 层（即图中的“候选值”模块）创建一个新的候选记忆向量 $\tilde{c}_t$，这是准备添加到细胞状态中的新内容。这部分的计算与常规 RNN 的计算非常相似。

$$
\tilde{c}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)
$$

### 第三步：更新细胞状态 (Cell State Update)

把旧状态 $c_{t-1}$ 更新为新状态 $c_t$。此步骤对应于图中细胞状态传送带上的核心操作：
-   首先，将旧状态 $c_{t-1}$ 与遗忘门 $f_t$ 的输出进行**逐元素相乘**，丢弃掉决定要忘记的部分。
-   然后，将输入门 $i_t$ 与候选记忆 $\tilde{c}_t$ **逐元素相乘**，筛选出需要加入的新信息。
-   最后，将这两部分**相加**，得到新的细胞状态 $c_t$。

$$
c_t = (f_t \odot c_{t-1}) + (i_t \odot \tilde{c}_t)
$$
> 逐元素乘法是指对两个相同维度的矩阵或向量中对应位置的元素进行相乘的操作，通常称为哈达玛积

### 第四步：输出门（Output Gate）


（1）**输出门**：一个 Sigmoid 层（“输出门”）决定了细胞状态的哪些部分将被输出。

$$
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
$$

（2）**计算隐藏状态**：将刚刚更新的细胞状态 $c_t$ 通过 `tanh` 函数（将其值规范化到 -1 和 1 之间），并将其与输出门 $o_t$ 的结果**逐元素相乘**，最终只输出我们决定的那部分信息作为隐藏状态 $h_t$。

$$
h_t = o_t \odot \tanh(c_t)
$$

这个 $h_t$ 将作为当前时间步的最终输出，并传递给下一个时间步。

## 缓解长距离依赖

关键在于**细胞状态 $c_t$ 的更新法则**。

在 RNN 中，梯度在时间步之间反向传播时，每一步都必须乘以同一个权重矩阵 $W$。当序列很长时，这种连乘操作（$W \cdot W \cdot W \dots$）极易导致梯度消失或爆炸。

而在 LSTM 中，梯度的传播路径被分成了两条。一条是通过隐藏状态 $h_t$ 传递的，与 RNN 类似，依然会经过激活函数的导数和权重矩阵，存在梯度衰减的风险。但另一条**是通过细胞状态 $c_t$ 传递的**。

我们来考察损失 $L$ 对前一时刻细胞状态 $c_{t-1}$ 的梯度，根据链式法则：

$$
\frac{\partial L}{\partial c_{t-1}} = \frac{\partial L}{\partial c_t} \frac{\partial c_t}{\partial c_{t-1}}
$$

其中，细胞状态的更新公式为 $c_t = (f_t \odot c_{t-1}) + (i_t \odot \tilde{c}_t)$。可以看到，$c_t$ 对 $c_{t-1}$ 的偏导数直接就是遗忘门 $f_t$（另一项与 $c_{t-1}$ 无关，导数为0）。因此：

$$
\frac{\partial L}{\partial c_{t-1}} = \frac{\partial L}{\partial c_t} \odot f_t
$$

这个关系非常关键。它表明，从 $t$ 时刻的细胞状态反向传播到 $t-1$ 时刻，梯度仅仅是**按元素乘以**了遗忘门 $f_t$ 的值，而**没有经过权重矩阵的乘法**。

如果将这个链条一直追溯到更早的 $k$ 时刻，梯度就变成了：

$$
\frac{\partial L}{\partial c_k} = \frac{\partial L}{\partial c_t} \odot (f_t \odot f_{t-1} \odot \dots \odot f_{k+1})
$$

这意味着：

（1）**梯度的“高速公路”**：从序列末端到开端的梯度传递，主要取决于一系列遗忘门 $f_t$ 的连乘。由于 $f_t$ 是一个独立的门控单元，它的值是在每次计算中动态生成的。如果模型在训练中发现某个早期信息非常重要，它可以通过学习将中间所有时间步的遗忘门 $f_t$ 的值都设置为接近 1。在这种情况下，梯度就可以几乎无衰减地从序列末端传播到序列开端，就像在一条高速公路上一样。

（2）**可学习的依赖关系**：与常规 RNN 的结构性缺陷不同，LSTM 将长距离依赖问题转化成了一个**可学习**的问题。模型可以通过优化损失函数，自行调整门控单元的参数，来决定哪些信息需要长期记忆（通过将 $f_t$ 设置为 $\approx 1$ 来“记住”），哪些信息可以被舍弃（通过将 $f_t$ 设置为 $\approx 0$ 来“遗忘”）。

因此 LSTM **可极大缓解**梯度消失问题。

## 手撕LSTM

其计算公式如下：

-   **遗忘门**: $f_t = \sigma(U_f x_t + W_f h_{t-1})$
-   **输入门**: $i_t = \sigma(U_i x_t + W_i h_{t-1})$
-   **候选记忆**: $\tilde{c}_t = \tanh(U_c x_t + W_c h_{t-1})$
-   **细胞状态更新**: $c_t = f_t \odot c_{t-1} + i_t \odot \tilde{c}_t$
-   **输出门**: $o_t = \sigma(U_o x_t + W_o h_{t-1})$
-   **隐藏状态更新**: $h_t = o_t \odot \tanh(c_t)$



### LSTM 实现层面的优化

在实际的深度学习框架（如 PyTorch、TensorFlow）中，为了提高计算效率，LSTM 的实现通常会进行一项优化。

回顾 LSTM 的计算过程，遗忘门、输入门、候选记忆和输出门都对拼接向量 $[h_{t-1}, x_t]$ 进行了独立的线性变换：
- $f_t$ : $W_f \cdot [h_{t-1}, x_t] + b_f$
- $i_t$ : $W_i \cdot [h_{t-1}, x_t] + b_i$
- $\tilde{c}_t$ : $W_c \cdot [h_{t-1}, x_t] + b_c$
- $o_t$ : $W_o \cdot [h_{t-1}, x_t] + b_o$

这相当于对同一个输入进行了四次独立的线性层计算。为了优化，框架会将这四个权重矩阵和偏置项在内部进行**拼接**：
- 将与输入 $x_t$ 相关的权重 $W_{fx}, W_{ix}, W_{cx}, W_{ox}$ 拼接成一个大的权重矩阵 $W_{x}$。
- 将与隐藏状态 $h_{t-1}$ 相关的权重 $W_{fh}, W_{ih}, W_{ch}, W_{oh}$ 拼接成一个大的权重矩阵 $W_{h}$。

这样，四次独立的矩阵乘法就可以被合并成两次更大规模的矩阵乘法，然后再将结果切分成四份，分别送入各自的激活函数。这种方式能更好地利用 GPU 的并行计算能力，提升运算速度。

# GRU

相比 LSTM 更易于训练。

## 主要改进

GRU 对 LSTM 做了两大核心改动：

（1）**合并细胞状态与隐藏状态**：GRU 不再区分细胞状态 $c_t$ 和隐藏状态 $h_t$。它只有一个状态向量 $h_t$ 在时间步之间传递。这个 $h_t$ 既包含了长期记忆，也作为当前时间步的输出，类似于 RNN 的结构。

（2）**简化门控结构**：GRU 将 LSTM 的三个门（遗忘、输入、输出）简化为了两个门：
-   **更新门（Update Gate, $z_t$）**: 它的作用类似于 LSTM 中耦合的遗忘门和输入门。它同时决定了要从前一状态 $h_{t-1}$ 中**保留**多少信息，以及要从当前的**候选状态**中接收多少新信息。
-   **重置门（Reset Gate, $r_t$）**: 决定在计算候选状态时，要**忽略**多少来自前一状态 $h_{t-1}$ 的信息。

这种设计使得每个隐藏单元能够自适应地捕捉不同时间尺度的依赖关系，从而更灵活地处理长短句法结构。

## 单元结构与公式

一个 GRU 单元如图 3-3 在 $t$ 时刻接收两个输入，当前输入 $x_t$ 和前一时刻的隐藏状态 $h_{t-1}$。然后，它通过内部的两个门，计算出新的隐藏状态 $h_t$。

![CUDA版本](images/3_2_2.svg)

其计算过程可以分解为以下四步：

（1）**重置门（$r_t$）**

决定如何将新输入 $x_t$ 与之前的记忆 $h_{t-1}$ 结合。这个门控制着哪些旧信息可以被用来计算“候选记忆”。

$$
r_t = \sigma(W_r \cdot [h_{t-1}, x_t] + b_r)
$$

（2）**更新门（$z_t$）**

控制前一时刻的状态信息 $h_{t-1}$ 有多少能够被直接带入到当前时刻。这与 LSTM 的遗忘门功能相似。

$$
z_t = \sigma(W_z \cdot [h_{t-1}, x_t] + b_z)
$$

（3）**候选记忆（$\tilde{h}_t$）**

计算当前时间步的候选隐藏状态。这个计算受到了**重置门** $r_t$ 的影响。$r_t$ 与 $h_{t-1}$ 逐元素相乘，如果 $r_t$ 的某个元素接近 0，则表示在计算候选记忆时，将完全忽略掉 $h_{t-1}$ 对应维度的信息。

$$
\tilde{h}_t = \tanh(W_h \cdot [r_t \odot h_{t-1}, x_t] + b_h)
$$

（4）**最终隐藏状态（$h_t$）**

结合更新门 $z_t$、旧状态 $h_{t-1}$ 和候选记忆 $\tilde{h}_t$，生成当前时间步的最终输出 $h_t$。
-   $z_t \odot h_{t-1}$：这部分表示对旧状态 $h_{t-1}$ 中需要**保留**的信息。
-   $(1-z_t) \odot \tilde{h}_t$：这部分表示从候选记忆 $\tilde{h}_t$ 中需要**选择**的新信息。

这个更新机制非常巧妙，更新门 $z_t$ 的值在 0 到 1 之间，可以看作是一个“开关”。当 $z_t$ 接近 1 时，模型则倾向于保留更多的旧信息 $h_{t-1}$；当 $z_t$ 接近 0 时，模型倾向于保留更多的新信息（候选记忆）。

$$
h_t = z_t \odot h_{t-1} + (1 - z_t) \odot \tilde{h}_t
$$

# LSTM 的常见变体

标准的 LSTM 结构本身已经非常强大，但在其发展过程中，研究者们也提出了一些有趣的变体，用于简化计算或增强性能。其中两种著名的变体是“窥孔连接”和“耦合的输入/遗忘门”。

## 窥孔连接（Peephole Connections）

在标准 LSTM 中，三个门（遗忘、输入、输出）的决策依据仅来自当前输入 $x_t$ 和前一时刻的隐藏状态 $h_{t-1}$。但这种机制有一个潜在的弱点，**门控单元无法直接“看到”它们控制的细胞状态** 。

特别是当输出门关闭时，隐藏状态 $h_{t-1}$ 接近于 0，此时门控单元失去了关于细胞内部状态的重要信息。为了解决这个问题，研究者提出了**窥孔连接**，允许门控单元直接访问细胞状态：

-   **遗忘门和输入门**：在做决策时，会“窥视” **前一时刻的细胞状态 $c_{t-1}$**。
-   **输出门**：在做决策时，会“窥视” **当前刚刚更新的细胞状态 $c_t$**。

实验表明，带有窥孔连接的 LSTM 在处理需要精确计时和计数的任务（如学习生成具有特定时间间隔的脉冲序列）时，性能显著优于标准 LSTM。

公式上的变化体现为，在计算每个门时，额外加入一个与细胞状态相关的项：
-   $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + V_f \odot c_{t-1} + b_f)$
-   $i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + V_i \odot c_{t-1} + b_i)$
-   $o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + V_o \odot c_t + b_o)$

其中 $V_f, V_i, V_o$ 是新增的对角权重矩阵，代表窥孔连接的权重。由于细胞状态的维度通常与隐藏状态一致，所以这里的运算通常是按元素相乘，这意味着每个门的激活只受到其对应细胞单元状态的影响，保持了计算的局部性。


### 耦合的输入与遗忘门（Coupled Input and Forget Gate）

**遗忘旧信息和写入新信息是紧密耦合的两个决策**。应该遗忘多少旧信息，恰恰是因为准备写入等量的新信息。

基于此，它将输入门和遗忘门合并为一个决策。不再单独计算输入门 $i_t$，而是直接令 $i_t = 1 - f_t$。当遗忘门 $f_t$ 的某个元素为 0.8（保留 80% 的旧信息）时，对应的输入门元素就必须是 0.2（只允许 20% 的新信息进入）。细胞状态的更新公式因此变得更加简洁：

$$
c_t = (f_t \odot c_{t-1}) + ((1 - f_t) \odot \tilde{c}_t)
$$

这种方式不仅使得模型逻辑更直观，还减少了模型的参数量。

除了验证 CIFG 的有效性外，该研究还对 LSTM 的架构进行了详尽的探索，得出了一些对工程实践极具价值的结论：

（1）**核心组件**：**遗忘门**和**输出激活函数**是 LSTM 中最关键的组件，移除它们会显著降低性能。

（2）**超参数独立性**：超参数之间几乎是独立的，这意味着可以单独调整学习率等参数，而无需进行复杂的组合搜索。

（3）**动量作用有限**：在在线随机梯度下降训练中，动量对 LSTM 的性能影响微乎其微。
