# 文本表示与词向量
## 初级分词技术(词元Token)

以中文分词为例
| 方法 | 原理 | 优势&#x2705; | 不足&#x26a0;&#xfe0f; |
|------|------|:----:|:----:|
| **人工词典** | 构建DAG，基于词典DP找到出现概率最大的分词结果 | easy、自定义词典解决oov | 过度依赖词典 |
| **统计学习** | 1. 序列标注:为每个字标注其在词的位置(B,M,E,Single) <br> 2. 隐马尔可夫(HMM):预测每个字最可能的位置 | 能识别新词解决oov | 效率低，不适用复杂场景 |
| **子词分词** | `BPE`：在原始的字符语料库上，迭代地将高频的相邻字节对合并成一个新的、更大的单元加入词表 | 平衡字和词，词汇表大小可控，高频词完整保留、低频词可被进一步拆分。大模型标准 | 语言学解释弱，依赖训练语料 |
> 未登录词（OOV, Out-Of-Vocabulary）是一个“相对概念”——相对某个词典/词表未被收录的词。对词典法来说，就是词典里没有；常见表现是被切成多个字或更小片段，需要通过维护词典（或引入其它模型）来修正。
### jieba实践
1. 人工词典
```py
import jieba
# user_dict.txt 三月七\n崩坏星穹铁道
# 未加载词典前的错误分词
text = "三月七是崩坏星穹铁道的看板娘"
print(f"精准模式: {jieba.lcut(text, cut_all=False)}")
#['三月', '七是', '崩坏', '星穹', '铁道', '的', '看板娘']
# 加载自定义词典
jieba.load_userdict("./user_dict.txt") 
print(f"加载词典后: {jieba.lcut(text, cut_all=False)}")
#['三月七', '是', '崩坏星穹铁道', '的', '看板娘']

```
2. HMM识别oov
```py
text = "我在Boss直聘找工作"

# 开启HMM（默认）
seg_list_hmm = jieba.lcut(text, HMM=True)
print(f"HMM开启: {seg_list_hmm}")
#['我', '在', 'Boss', '直聘', '找', '工作']
# 关闭HMM
seg_list_no_hmm = jieba.lcut(text, HMM=False)
print(f"HMM关闭: {seg_list_no_hmm}")
#['我', '在', 'Boss', '直', '聘', '找', '工作']
```
3. 词性标注：词典+HMM
在已经加载自定义词典```三月七\n崩坏星穹铁道```时：
```py
import jieba.posseg as pseg
# HMM=False 强制只使用词典和动态规划
words = pseg.lcut(text, HMM=False)
print(f"默认词性输出: {words}")
#[pair('三月七', 'x'), pair('是', 'v'), pair('崩坏星穹铁道', 'x'), pair('的', 'uj'), pair('看', 'v'), pair(' 板', 'ng'), pair('娘', 'n')]
```
可以通过修改词频干预分词路径，比如将"三""月"分别取出，可以修改词典为
```
月 10000000 n
崩坏星穹铁道 nz
看板娘 nz
```
输出```[pair('三', 'm'), pair('月', 'n'), pair('七', 'm'), pair('是', 'v'), pair('崩坏星穹铁道', 'nz'), pair('的', 'uj'), pair('看板娘', 'nz')]```。实验后发现，如果只包含单个字，将"月"的词频置为${10^8}$以下即无法拆分，而是将“三月”作为一个整体输出。但如果添加一项词频${10^6}$的“三”，则 "月"的词频置为${10^7}$即可拆分。这说明jieba在进行动态规划计算时，单独的数字如```三```可能默认词频很低，而只有```三+月```这条路径的概率大于```三月```这条路径，才会优先选择前者。

## 词向量表示
### 离散表示
1. 独热编码：收集所有唯一词，向量表示是否包含。**主要缺陷**：任意两个不含公共词语的向量正交，无法表示语义相关性。
2. **词袋模型（BoW，Bag-of-Words）**：
+ 忽略文本中的词序和语法，将其仅仅视作一个装满词的"袋子"，用袋子中每个词出现的统计量（是否出现、出现次数、出现频率）来表示整个文档，使用词频（TF）可以缓解文档长度不同带来的频数差异。
    ```
    词汇表=[原神,和,崩坏,绝区零,是,米哈游,的,IP]
    原神和崩坏是米哈游的IP [1,1,1,0,1,1,1]
    原神和绝区零是米哈游的IP [1,1,0,1,1,1,1]
    ```
+ 余弦相似度：${cos(\theta)=\frac{|A·B|}{|A||B|}}$ 上述两句带入计算为0.8，说明很接近
+ 优点：不关心词序，适于文本分类等任务。  
+ **主要缺陷**：
    1. 丢失词序，无法区分"我爱你"和"你爱我"的语义差异。
    2. 未考虑词的重要性：像"的"、"是"这类在所有文档中都频繁出现的停用词，会获得很高的频次，但它们对区分文档主题几乎没有贡献，反而会形成干扰。
    > 停用词：信息检索过程中为节省存储空间、提高搜索效率而自动过滤的字词，主要应用于 搜索引擎领域。可分为功能词与高频词汇两类：前者包括冠词、介词等无实际含义的结构性词语（如the、on、的）；后者如某些应用广泛但降低搜索精度的词汇。
3. **N元语法（N-gram模型）** LLM鼻祖
统计连续词组，核心基于马尔可夫假设，认为一个词出现的概率只取决于它前面 N-1 个词。N=1为词袋，GPT等本质可视为N很大的N-gram。
    ```
    我爱玩原神
    Bigram特征(N=2):  {"我爱","爱玩","玩原神"}
    Trigram特征(N=3): {"我爱玩","爱玩原神"}
    ```
+ **主要缺陷**：维度灾难；数据稀疏：多数词的组合在语料中罕见。
4. **TF-IDF** 一种**加权技术**，提升文档在向量空间中的区分度，解决"常见词权重过高"导致文档混淆的问题。基本思想是：一个词在当前文档里越常见，但在其他文档里越罕见，其权重就越高。
### 序号化表示（现代主流）
1. 基本思想：进行最少的预处理。不再通过复杂的特征工程（如计算 TF-IDF）来告诉模型哪些词重要。只把文本转换成最基础的整数 ID 序列，然后把 **"学习词语的含义和重要性"这个更复杂的任务，交给模型自己去完成**。
2. 序号化（整数编码）过程：将分词后的词元序列转换为整数序列
    + 构建词典：类似独热编码，但通常是字级别或是子词级别，而不是词级别的。
    + 增加特殊词元：在词典中加入一些有特殊功能的 Token，至少包括：
        - [PAD] (Padding)：用于填充。批处理（Batch Processing）内的所有句子必须长度相同。短句子会用[PAD]填充到与最长句子一致的长度。
        - [UNK] (Unknown)：表示所有在词典中未出现过的词。
        - 特定任务的特殊词元：[CLS] (Classification), [SEP] (Separator) 
    + ID 映射：将文本序列中的每个词元（字/子词）直接映射为其在词典中的整数 ID。
    + 利用已有的预训练模型的词典
    ```
    例如：
    词典{'[PAD]': 0, '[UNK]': 1, '我': 2, '最': 3, '爱': 4, '玩': 5, '崩': 6, '坏': 7}
    我爱玩：[2,4,5,0,0]
    最爱崩3：[3,4,6,1,0]
    我玩崩坏3：[2,5,6,7,1]
    ```
    把这3个数组concat就可以输入模型。
> 序号化**本身并未解决语义鸿沟**，其整数ID（如 2 和 3）不具备数学意义。它的真正价值是作为后续嵌入层的输入。嵌入层会将这些ID查询并映射为低维、稠密的浮点数向量（即词向量），而这个映射关系本身是在模型训练中学习出来的

## 分布式表示
### 理想的词向量
语义蕴含：两个词经常在相似的上下文共同出现，则其向量在空间上应接近；
低维稠密
### 主题模型（LAS）：全局文档统计
假设一篇文档由多个"主题"按一定比例混合而成，而一个主题又由多个"词语"按一定概率组成。因此**词向量可以用它与各个主题关联强度表示**
1. 核心技术：SVD矩阵分解$X_{m×n}=W_{m×k}×H_{k×n}$
+ $X_{m×n}$ 词-文档矩阵：$X(i,j)$代表第$j$篇文档第$i$个词的重要性权重。很稀疏
+ $k$ 超参数，期望发现的主题数
+ $W_{m×k}$ 词主题矩阵，每一行表示这个词与$k$个主题的关联度
+ $H_{k×n}$ 文档主题矩阵，每一列表示这篇文档属于$k$个主题其一的置信度
2. 聚类视角理解：文档聚类（每个文档属于各个主题的置信度）、词语倾向性、同一主题的词语在这个主题维度下权重都较高，向量接近——语义相关性
3. 局限性：矩阵分解计算代价高、全局统计难把握更精细语义关系、先统计在分解的流程**难集成——进行端到端联合训练**
### Word2Vec
聚焦于词语的局部上下文。它的思想来源于语言学中的分布式假设，即一个词的含义，由其上下文中的词语所决定。两个词上下文相近则语义相近。
1. 概述：浅层神经网络模型——移除非线性隐藏层提升，投影与输出层直接相连。需区分目的与手段
    + 最终目的：**获得高质量的词向量查询表$W_{in}$**
    + 手段：设计任务为根据上下文预测中心词，在这个过程中查询表为进行训练和优化的模型参数
2. 过程：$W_{in}$被随机初始化，根据输入的单词ID从$W_{in}$中获取对应的行向量进行计算。$W_{in}$在后续通过下述模型的预测任务不断优化。
3. 模型 批大小$B$ 上下文单词数$S$ 词典大小$|V|$ 词向量维度$D$
    + CBOW:上下文预测中心词。输入上下文窗口所有词(B,S)
        ```mermaid
        graph TD
        A[输入] --> B[词向量转换]--> C[上下文向量聚合] --> D[输出得分计算] --> E[损失计算]
        ```
    + Skip-gram：中心词预测上下文。输入中心词(B,1)
        ```mermaid
        graph TD
        A[输入] --> B[词向量转换]--> C[输出得分计算] --> D[损失计算]
        ```
        词向量转换：$v_{c-k}=W_{in}w_{c-k}$

        上下文聚合：上下文窗口所有词向量取均值,得到$h$

        输出得分：$z_c=W^T_{out}h$

        损失计算：
        
        - CBOW：使用 Softmax 将得分转换为概率分布，然后计算与真实中心词之间的交叉熵损失。反向传播更新$W_{in},W_{out}$,最终得到训练好的$W_{in}$

            min $J=-u^T_ch+log\sum_{j=1}^{|V|}exp(-u^T_jh)$

            $u^T_c$是目标中心词的输出向量
        - Skip-gram：使用 Softmax 将得分转换为概率分布，然后计算与真实中心词之间的交叉熵损失。反向传播更新$W_{in},W_{out}$,最终得到训练好的$W_{in}$

            min $J=-\sum_{j=0,j≠m}^{2m}u^T_{c-m+j}v_c+2mlog\sum_{k=1}^{|V|}exp(-u^T_kv_c)$

            $u^T_{c-m+j}$是上下文词的输出向量,v_c是中心词的输入向量
        > Skip-gram为每个"中心词-上下文词"对都创建了一个独立的学习任务，这使得它能够更好地学习到词与词之间更精细的关系。在处理低频词和大数据集时，通常能得到质量更高的词向量，但由于其任务量是CBOW的S倍，训练速度相对较慢。
    + 滑动窗口机制：生成大量高度重叠的训练样本
    + 训练目标：最大化 score=$x·u_{target}$或score=$v_c·u_{target}$，其中$u_{target}$为输出矩阵$W_{out}$对应词的行向量，越大二者越接近。
    > 实际训练中，为避免对整个词表进行 Softmax 归一化带来的高开销，可以用 Hierarchical Softmax 与负采样等近似方法加速训练
4. 局限性：**产生的是静态向量**
+ 应用时上下文无关：词典中的任意一个词只会有一个固定的向量表示，是在整个语料库中训练得到的“平均语义”，**无法解决一词多义**
 ## Gensim实战
 1. 工作流：基于BoW的模型（非NN）:
    + 准备语料：将原始的文本文档进行分词，并整理成Gensim要求的格式——嵌套列表，每个子列表代表一篇独立的文档。

    + 创建词典：遍历所有分词后的文档，创建一个词典，将每个唯一的词元（Token）映射到一个从 0 开始的整数 ID。

    + 词袋化：使用创建好的词典，将每一篇文档转换为其稀疏的词袋（BoW）向量。这个向量**只记录文档中出现的词的 ID 及其频次**，格式为 [(token_id, frequency), ...]。
 2. TF-IDF 见```2.1.py```
 3. LDA与文档主题挖掘 见```2.2py```
 4. Word2Vec 见```2.3py``